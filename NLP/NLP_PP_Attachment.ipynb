{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds2o8jo0RAah"
      },
      "source": [
        "# NLP Syntactic Parsing exercise: Sensible PP attachment\n",
        "\n",
        "In this exercise, we will learn about **POS tagging** and **dependency parsing** and study the well-known **PP attachment problem**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction and POS tagging\n",
        "\n",
        "First, let's take a look at spaCy's Part-of-Speech (POS) tagging and dependency parsing abilities. Here's how we load a sentence into a spaCy document object and view its dependency parse:"
      ],
      "metadata": {
        "id": "pOhqQfIWcaLU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lob_YUywfMu5",
        "outputId": "8bc26f89-8587-485f-e82d-b2cda7830794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! python -m spacy download en_core_web_sm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En8gggO1ToI_",
        "outputId": "17f80c2c-7f2b-4006-efd8-a2da9f7963db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "test_doc = nlp('I write code.')\n",
        "displacy.render(test_doc, jupyter=True,options={'compact': True})\n",
        "# Note: you can add options={'compact': True} to get a more compact image"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"9f92e64f391040fa91be793561ebf4ab-0\" class=\"displacy\" width=\"500\" height=\"212.0\" direction=\"ltr\" style=\"max-width: none; height: 212.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">write</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"122.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">code.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f92e64f391040fa91be793561ebf4ab-0-0\" stroke-width=\"2px\" d=\"M62,77.0 62,52.0 200.0,52.0 200.0,77.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f92e64f391040fa91be793561ebf4ab-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M62,79.0 L58,71.0 66,71.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-9f92e64f391040fa91be793561ebf4ab-0-1\" stroke-width=\"2px\" d=\"M212,77.0 212,52.0 350.0,52.0 350.0,77.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-9f92e64f391040fa91be793561ebf4ab-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M350.0,79.0 L354.0,71.0 346.0,71.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXiVkSQJUM2B"
      },
      "source": [
        "spaCy also tokenizes the sentence for you. You can view tokens and their POS tags as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bc9NNT1UL_5",
        "outputId": "9121e7ec-d3fb-4b37-d9b8-348767de9180",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print([(token, token.pos_) for token in test_doc])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(I, 'PRON'), (write, 'VERB'), (code, 'NOUN'), (., 'PUNCT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAWRioM1VPYJ"
      },
      "source": [
        "Now let's try applying this to a real dataset. NLTK includes an API for accessing many free open textual corpora, including the Project Gutenberg collection of public domain books. We'll load an array of the sentences of Jane Austen's 1811 novel *Sense and Sensibility* for our tests:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMAyOna7Ovjj",
        "outputId": "b9506238-44df-4cc3-ecb3-57f369441473",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import gutenberg\n",
        "sentences = gutenberg.sents('austen-sense.txt')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAbizPB7PXkA"
      },
      "source": [
        "## Question 1\n",
        "How many sentences are in the novel?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "len(sentences)"
      ],
      "metadata": {
        "id": "Uv4ctN4vPL8z",
        "outputId": "0172f50c-8581-4e37-a52d-96ebf4d2907b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4999"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is 4,999 sentences in the novel."
      ],
      "metadata": {
        "id": "2LfwYFs8WJDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2\n",
        "Create a list of spaCy parsed documents from the sentences.  *Hint:* you need to reconstruct the original sentences. *Hint:* `sentences` is iterable"
      ],
      "metadata": {
        "id": "aC30aY4qOwhN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "nlps = []\n",
        "for sentence in sentences:\n",
        "  nlp_temp = nlp(' '.join(sentence))\n",
        "  nlps.append(nlp_temp)\n"
      ],
      "metadata": {
        "id": "3eXkx0_BPN1L"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3\n",
        "Create a flat list of tokens in all of the documents.  How many unique lowercase tokens are there?"
      ],
      "metadata": {
        "id": "5hMkXO8OQkIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "all_tokens = []\n",
        "for parsed_doc in nlps:\n",
        "  for token in parsed_doc :\n",
        "    all_tokens.append(token)\n"
      ],
      "metadata": {
        "id": "Sk1KjNZfRjb6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.unique([token.lower_ for token in all_tokens]).shape"
      ],
      "metadata": {
        "id": "muhFgA1aZXNK",
        "outputId": "44f7c778-37d4-42d2-e73f-20bb3ab19afe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6354,)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6,354 unique lowercase tokens."
      ],
      "metadata": {
        "id": "Ed70bgo9cJST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4\n",
        "What are the five most common lowercase verbs in the novel counting different inflections separately?"
      ],
      "metadata": {
        "id": "q_CdqoFqOzbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_verbs = []\n",
        "for token in all_tokens:\n",
        "  if token.pos_ == 'VERB':\n",
        "    all_verbs.append(token.text)"
      ],
      "metadata": {
        "id": "bFr4SEvAfRpT"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "from collections import Counter\n",
        "counter = Counter(all_verbs)"
      ],
      "metadata": {
        "id": "OJK6aTlJPOYD"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter.most_common(5)"
      ],
      "metadata": {
        "id": "2w_O8UBSfqIA",
        "outputId": "28811e96-21b0-4e61-e612-2e06ae3cc245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('said', 397), ('had', 246), ('know', 230), ('have', 224), ('think', 208)]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The five most common lowercase verb are : 'said','had','know','have','think'."
      ],
      "metadata": {
        "id": "EceCqOuLf9l0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5\n",
        "What are the five most common verbal lemmas (base forms of verbs)?"
      ],
      "metadata": {
        "id": "a0xwUGhcPDuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens[1].lemma_"
      ],
      "metadata": {
        "id": "TVcnocwXgLog",
        "outputId": "bc14019e-4c72-41df-8ef9-49bc5d3b50d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sense'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here\n",
        "all_verbs_lemma = []\n",
        "for token in all_tokens:\n",
        "  if token.pos_ == 'VERB':\n",
        "    all_verbs_lemma.append(token.lemma_)"
      ],
      "metadata": {
        "id": "5VWBB-aFPO57"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter_lemma = Counter(all_verbs_lemma)"
      ],
      "metadata": {
        "id": "ND7mZFahgY7s"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counter_lemma.most_common(5)"
      ],
      "metadata": {
        "id": "jaAcYAeogb6A",
        "outputId": "e9544851-f430-4b6d-cc7c-31071fc396d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('say', 608), ('have', 556), ('know', 385), ('see', 383), ('do', 355)]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 5 most common verbal lemmas are : 'say','have','know','see','do'."
      ],
      "metadata": {
        "id": "fc8G7xyEgg1O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujsWoL1NZ5nG"
      },
      "source": [
        "## Dependency parsing and PP attachment\n",
        "\n",
        "As we saw above, spaCy also generates dependency parses that we can plot. These represent the grammatical relations that connect the different words and phrases in a sentence.\n",
        "\n",
        "For the next task, we will consider how verbs and prepositional phrases can be related in sentences. (A *prepositional phrase* or *PP* is a phrase like \"in the house\", \"on the table\", \"with my friend\" which is headed by a prepisition like \"in\", \"on\", \"with\" ...)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6\n",
        "What is the difference between the prepositional phrases in the sentences in (A) and those in (B)? Plot their dependency parses with `displacy.render` and look for a difference in structure.\n",
        "\n",
        "(A)\n",
        "  * I eat an apple in my room.\n",
        "  * We listen to music at the theater.\n",
        "  * John visited Brazil with his friend.\n",
        "  \n",
        "(B)\n",
        "  * I see a fly in my soup.\n",
        "  * She knows the man at the store.\n",
        "  * I photographed a man with a hat.\n",
        "\n",
        "**Note:** it's possible that some of the sentences above will not be parsed properly.  Use your judgement and different parsings to differentiate between the groups."
      ],
      "metadata": {
        "id": "DyRIzFLfhXd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "Dp3WJ-KcT5n8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cqzisHbfIZG"
      },
      "source": [
        "As you can imagine, it is not simple for the parser to decide where the prepositional phrase should be attached -- this is the **PP attachment problem**. Let's evaluate spaCy's default behavior towards PP attachment on our *Sense and Sensibility* corpus:\n",
        "\n",
        "## Question 7\n",
        "Create tuples (verb lemma, preposition lemma) for prepositional phrases attached to the verb (like (A) above). *Hint:* for a spaCy token object `token`, you can get its children with `token.children` and the child's relation to it with `child.dep_`. What are five most common (verb lemma, preposition lemma) pairs in the novel?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "Of_92e0GUp3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8\n",
        "Do the same where the prepositional phrase is attached to the verb's object (case (B)). What are the five most common (verb lemma, preposition lemma) pairs in this case? **Hint:** what should be the verb's child's dependency type? what should be the child child's dependency type?"
      ],
      "metadata": {
        "id": "IAP91eSoUfcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "8mAnBuWUUqNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus question\n",
        "Look at a few random sentences from the corpus that are parsed as (A) or (B). Do you agree with the given parsing? Why or why not?"
      ],
      "metadata": {
        "id": "tvzCLtbzUelT"
      }
    }
  ]
}