{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSWzy0w5ESWk"
      },
      "source": [
        "# NLP Preprocessing exercise: Building a \"little stemmer\"\n",
        "\n",
        "For this exercise, we will take a sample of Antoine de Saint-ExupÃ©ry's novella *The Little Prince* and use it to demonstrate tokenization and stemming.\n",
        "\n",
        "Here is your sample text, which appears at the beginning of the book:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38EI6SxlDzZR"
      },
      "source": [
        "text = \"\"\"\n",
        "Once when I was six years old I saw a magnificent picture in a book, called True Stories from Nature, about the primeval forest. It was a picture of a boa constrictor in the act of swallowing an animal. Here is a copy of the drawing.\n",
        "Boa\n",
        "In the book it said: \"Boa constrictors swallow their prey whole, without chewing it. After that they are not able to move, and they sleep through the six months that they need for digestion.\"\n",
        "I pondered deeply, then, over the adventures of the jungle. And after some work with a colored pencil I succeeded in making my first drawing. My Drawing Number One. It looked something like this:\n",
        "Hat\n",
        "I showed my masterpiece to the grown-ups, and asked them whether the drawing frightened them.\n",
        "But they answered: \"Frighten? Why should any one be frightened by a hat?\"\n",
        "My drawing was not a picture of a hat. It was a picture of a boa constrictor digesting an elephant. But since the grown-ups were not able to understand it, I made another drawing: I drew the inside of a boa constrictor, so that the grown-ups could see it clearly. They always need to have things explained. My Drawing Number Two looked like this:\n",
        "Elephant inside the boa\n",
        "The grown-ups' response, this time, was to advise me to lay aside my drawings of boa constrictors, whether from the inside or the outside, and devote myself instead to geography, history, arithmetic, and grammar. That is why, at the age of six, I gave up what might have been a magnificent career as a painter. I had been disheartened by the failure of my Drawing Number One and my Drawing Number Two. Grown-ups never understand anything by themselves, and it is tiresome for children to be always and forever explaining things to them.\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaxfIOKzFUwy"
      },
      "source": [
        "First let's use NLTK's build-in functions to tokenize and stem this text. First convert the given text into an array of lowercase tokens using the NLTK functions word_tokenize and PorterStemmer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "word_tokenized = word_tokenize(text)"
      ],
      "metadata": {
        "id": "u1ptfanx-2Jm",
        "outputId": "eea711cf-d548-414a-e068-576d141fa3be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "words_stemmed = [stemmer.stem(word) for word in word_tokenized]"
      ],
      "metadata": {
        "id": "f5F44ptc_zKS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ4Hh5JpIM7w"
      },
      "source": [
        "**Questions:**\n",
        "  1. How many unique tokens are there in the text?\n",
        "\n",
        "  1. How many unique stemmed tokens are in the text? How many lowercase stemmed tokens?\n",
        "  \n",
        "  1. What are some examples of words that have surprising stemmed forms? Can you explain why? Answer from a linguistic point of view"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1.\n",
        "import numpy as np\n",
        "np.unique(word_tokenized).shape"
      ],
      "metadata": {
        "id": "GaCfqOgtBCa0",
        "outputId": "37091cc7-1bc0-4956-a103-1571c3c25039",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(170,)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 170 unique tolens in the text."
      ],
      "metadata": {
        "id": "gobwt7jtBLwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#2.\n",
        "np.unique(words_stemmed).shape"
      ],
      "metadata": {
        "id": "YYCRScHPBRn-",
        "outputId": "a2b62674-263c-4ef4-bcd0-f024f4649262",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(149,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 149 unique stemmed tokens in the text."
      ],
      "metadata": {
        "id": "7ByxtVZBBX3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(list(map(lambda x : x.islower(), words_stemmed)), return_counts = True)"
      ],
      "metadata": {
        "id": "-S0Xze9_BfJU",
        "outputId": "d91fa1e3-ee53-4ee0-9dc2-df0f3dec689a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([False,  True]), array([ 48, 305]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Those words have surprising stemmed forms. This is because in stemming we keep for each word only the word radical so it can lead to unknown words."
      ],
      "metadata": {
        "id": "fC3JjnjKCkw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#3.\n",
        "words_stemmed[0], words_stemmed[3], words_stemmed[10]"
      ],
      "metadata": {
        "id": "S2pctDDPCSxH",
        "outputId": "836796ec-83e1-48c9-c1ae-5cb18e254fbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('onc', 'wa', 'magnific')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is 305 lowercase stemmed tokens."
      ],
      "metadata": {
        "id": "mFbeYcIpCONS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jec0fStwKy5q"
      },
      "source": [
        "Now let's try writing our own stemmer. Write a function which takes in a token and returns its stem, by removing common English suffixes (e.g. remove the suffix -ed as in *listened* -> *listen*). Handle at least four such suffixes in English. Then use this custom stemmer to convert the given text to an array of **lowercase stemmed tokens**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def mystemmer(token):\n",
        "  suffixes = ['ed','ing','ment','s']\n",
        "  for suffix in suffixes:\n",
        "    if token.endswith(suffix):\n",
        "      return token.removesuffix(suffix).lower()\n",
        "  return token.lower()\n",
        "\n"
      ],
      "metadata": {
        "id": "5yvN_9NHDQd0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_words_stemmed = [mystemmer(word) for word in word_tokenized]"
      ],
      "metadata": {
        "id": "F5q_5IJWIt3P"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXjxQgsoK8EM"
      },
      "source": [
        "**Questions:**\n",
        "  4. What are some examples where  your stemmer on the text differs from the PorterStemmer?\n",
        "  \n",
        "  5. Can you explain why the differences occur?\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.\n",
        "word_not_common = [word for word in my_words_stemmed if word not in words_stemmed]\n",
        "word_not_common[:10]"
      ],
      "metadata": {
        "id": "0JhppDWOI9Y_",
        "outputId": "692fcfd7-7bad-4241-fe65-cd4a4c39b075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['once',\n",
              " 'magnificent',\n",
              " 'picture',\n",
              " 'storie',\n",
              " 'nature',\n",
              " 'primeval',\n",
              " 'picture',\n",
              " 'animal',\n",
              " 'copy',\n",
              " 'able']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "'once',\n",
        " 'magnificent',\n",
        " 'picture',\n",
        " 'storie',\n",
        " 'nature',\n",
        " 'primeval',\n",
        " 'picture',\n",
        " 'animal',\n",
        " 'copy',\n",
        " 'able'"
      ],
      "metadata": {
        "id": "skJNzZSXJuL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. The difference occurs because we are not considering all the possible suffixes in our own function stemmer."
      ],
      "metadata": {
        "id": "Pnrtv9YIJ1GW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0Rh-uzyjntv"
      },
      "source": [
        "Finally, we will use the library Spacy to lemmatize the text and compare the output to the stemming performed above. First we load the default Spacy model for English:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ogFNrBTiv-B"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Note: You may need to run the following command first to download the model:\n",
        "# ! python -m spacy download en_core_web_sm"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGI9GPeKj0bS"
      },
      "source": [
        "This contains Spacy's saved data about how to process English text. Now we will use this to lemmatize:\n",
        "\n",
        "**Question:**\n",
        "  6. Lemmatize the text and output an array of lemmatized tokens - **return lower cased tokens**. How many unique lemmas are in the text? Hint: Use *nlp(text)* as a Python iterator. Each item in the iterator has an attribute *.lemma_*.\n",
        "\n",
        "\n",
        "  7. What is an example of a word which has different lemmatized and stemmed forms? Why? Answer from a linguistic point of view"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = [1,2,3,3].remove(3)"
      ],
      "metadata": {
        "id": "_I7RZ1R7PSwC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l"
      ],
      "metadata": {
        "id": "_kse3Ca5PZvN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.\n",
        "doc= nlp(text)\n",
        "lemmatized_word = [token.lemma_.lower() for token in doc]"
      ],
      "metadata": {
        "id": "kPEU9jXnKL_U"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(lemmatized_word).shape"
      ],
      "metadata": {
        "id": "HGfspr56LEnm",
        "outputId": "527a2226-b0c1-4863-ebe4-60257f11325a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(141,)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "141 unique lemmas are in the text."
      ],
      "metadata": {
        "id": "d4AaankXOI9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#7.\n",
        "not_common_stemlem = [word for word in lemmatized_word if word not in words_stemmed]"
      ],
      "metadata": {
        "id": "APnNYUibOG9Y"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "not_common_stemlem[:10]"
      ],
      "metadata": {
        "id": "uK5hNaawOaG7",
        "outputId": "093e7dda-36fc-41b2-b50f-5921c4073919",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " 'once',\n",
              " 'magnificent',\n",
              " 'picture',\n",
              " 'story',\n",
              " 'nature',\n",
              " 'primeval',\n",
              " 'picture',\n",
              " 'animal',\n",
              " 'copy']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization reduce each word into a common base but in a dictionary form so each word will have a meaning.  It also changes word based on its intended meaning."
      ],
      "metadata": {
        "id": "eEpkXS7qRz-B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9ss39nRESBve"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}